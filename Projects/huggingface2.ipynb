{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:29:19.458141Z","iopub.execute_input":"2025-10-24T16:29:19.458503Z","iopub.status.idle":"2025-10-24T16:29:19.736877Z","shell.execute_reply.started":"2025-10-24T16:29:19.458476Z","shell.execute_reply":"2025-10-24T16:29:19.736159Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"https://huggingface.co/openai-community/gpt2","metadata":{}},{"cell_type":"code","source":"#clearing cache since the import from transformer was not working\n# Run in a Kaggle notebook cell\n!pip install -U transformers huggingface_hub\n!rm -rf /root/.cache/huggingface/hub /root/.cache/huggingface/transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:37:51.079759Z","iopub.execute_input":"2025-10-24T16:37:51.080425Z","iopub.status.idle":"2025-10-24T16:38:03.732571Z","shell.execute_reply.started":"2025-10-24T16:37:51.080393Z","shell.execute_reply":"2025-10-24T16:38:03.731447Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.20.0)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.20.0\n    Uninstalling huggingface-hub-0.20.0:\n      Successfully uninstalled huggingface-hub-0.20.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.35.2\n    Uninstalling transformers-4.35.2:\n      Successfully uninstalled transformers-4.35.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#to restart the kernal\n#import os, signal\n#os.kill(os.getpid(), signal.SIGKILL)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T16:40:05.888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:40:23.252359Z","iopub.execute_input":"2025-10-24T16:40:23.252628Z","iopub.status.idle":"2025-10-24T16:40:28.388674Z","shell.execute_reply.started":"2025-10-24T16:40:23.252605Z","shell.execute_reply":"2025-10-24T16:40:28.387977Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n\n\n# Load model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Create the text generation pipeline\ntg_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Input text\ntext = \"Once upon a time in Bengaluru,\"\n\n# Generate text using the pipeline\nanswer = tg_pipeline(\n    text,\n    max_length=100,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.7\n)\n\n# Print the generated text\nprint(answer[0]['generated_text'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:51:00.881753Z","iopub.execute_input":"2025-10-24T16:51:00.882267Z","iopub.status.idle":"2025-10-24T16:51:05.185896Z","shell.execute_reply.started":"2025-10-24T16:51:00.882242Z","shell.execute_reply":"2025-10-24T16:51:05.185286Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nBoth `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time in Bengaluru, for the first time in its history, the city was able to achieve independence from the British at sea with the help of the Indian Navy, the first of which was the USS Hindenburg.\n\nThe Hindenburg was the first of the three warships to make landfall in Bengaluru. It was the first ship to make landfall in Bengaluru since the British fleet at sea in the year 1300 in Canada.\n\nIt was the first ship to make landfall in Bengaluru since the British fleet at sea in the year 1300 in Canada.\n\nThe Hindenburg, with its nine hulls and four superstructure, were the first of the three vessels to make landfall in Bengaluru.\n\nIt was the first ship to make landfall in Bengaluru since the British fleet at sea in the year 1300 in Canada.\n\nThe Hindenburg, with its nine hulls and four superstructure, were the first of the three vessels to make landfall in Bengaluru.\n\nIt was the first ship to make landfall in Bengaluru since the British fleet at sea in the year 1300 in Canada.\n\nThe Hindenburg, with its nine hulls and four superstructure, were the first of the three vessels to make landfall in Bengaluru.\n\nIt was the first ship\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Generate text using the pipeline\nanswer = tg_pipeline(\n    text,\n    max_length=10,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.2\n)\n\n# Print the generated text\nprint(answer[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:57:00.993741Z","iopub.execute_input":"2025-10-24T16:57:00.994012Z","iopub.status.idle":"2025-10-24T16:57:03.285967Z","shell.execute_reply.started":"2025-10-24T16:57:00.993989Z","shell.execute_reply":"2025-10-24T16:57:03.285319Z"}},"outputs":[{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time in Bengaluru, the city was a bustling metropolis, with a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving textile industry, a thriving\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\nanswer = tg_pipeline(\n    text,\n    max_length=20,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.3,  # Increase randomness\n    top_k=50,         # Limit to top 50 likely tokens\n    top_p=0.95,       # Nucleus sampling\n    no_repeat_ngram_size=3  # Prevent repeating 3-word phrases\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:59:40.059605Z","iopub.execute_input":"2025-10-24T16:59:40.060176Z","iopub.status.idle":"2025-10-24T16:59:41.023574Z","shell.execute_reply.started":"2025-10-24T16:59:40.060149Z","shell.execute_reply":"2025-10-24T16:59:41.022966Z"}},"outputs":[{"name":"stderr","text":"Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\n# Print the generated text\nprint(answer[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T16:59:43.810209Z","iopub.execute_input":"2025-10-24T16:59:43.810939Z","iopub.status.idle":"2025-10-24T16:59:43.815019Z","shell.execute_reply.started":"2025-10-24T16:59:43.810917Z","shell.execute_reply":"2025-10-24T16:59:43.814202Z"}},"outputs":[{"name":"stdout","text":"Once upon a time in Bengaluru, the city's population of over 1.5 million was estimated at around 2.5 lakh.\n\nThe city's government has been trying to build a new railway line to connect the city to the rest of the country. The project is expected to cost Rs 1,000 crore.\n (Source: PTI)\n\nIn the past, the railway line has been built in the city. However, the project has been delayed due to the lack of infrastructure.\n.\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}